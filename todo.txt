May 5th 2021
------------
1. fix the rgb - nan/inf issue for non-parallel env
2. then try on-policy algorithm say PPO for navigation task (task_obs and rgb_obs)
3. then try to incorporate pfnet into rl agent

May 12th 2021
-------------
1. fix ppo with rgb_obs
2. test pretrained pfnet
3. check if end to end works or need any changes


end project goals:
1. reproduce pfnet on housing dataset
2. fine tuned it to igibson
3. use pretrained pfnet with rl agent ppo -> with better representation of particles
4. end-to-end pfnet + rl agent
5. baseline comparision

May 19th 2021
-------------
1. collect stats for random start position with random policy gaussian and uniform init distribution with/without transition noise
2. use tf profiling or some tool to narrow where the code is taking time
3. change local map for loop into batches
4. decrease batch size and number of steps

May 26th 2021
-------------
goal: better gaussian with noise
1. normalize reward in range [-10, 0]
2. provide est_pose to gt_pose to train rl_agent
3. plot mse per step metric for random and rl agent -
  a) different timesteps
  b) different number particles
  c) with and without transition noise
  d) different init covariance
4. try with large and smaller  [e-3, e-5] learning rates for sac

June 2nd 2021
-------------
0. plot avg mse per episode and end mse (> 50 eval eps, 150 eps steps plot unnormalized mse)
1. collect manual trajectory to cross check pfnet (discrete/continuous)
	- fine tuned vs non-finetuned
2. for rnd_agent
   a) fine tuned vs non-finetuned pfnet
   b) random starting position (say >100) with random maps/rooms
   c) different num_particles (500, 1000, 2000)
   d) different init distribution (gaussian, uniform)

$ nohup python -u test_pfnet.py \
--root_dir=./random/igibson_pfnet/run1 \
--pfnet_loadpath=./pfnetwork/checkpoints/pfnet_igibson_data/checkpoint_87_5.830/pfnet_checkpoint \
--obs_mode='rgb-depth' \
--num_eval_samples=50 \
--testfiles=./plots_data/random/*.tfrecord \
--batch_size=4 \
--init_particles_distr='gaussian' \
--init_particles_std '0.15' '0.523599' \
--particles_range=100 \
--num_particles=500 \
--transition_std '0.02' '0.0872665' \
--resample=true \
--alpha_resample_ratio=0.8 \
--global_map_size '4000' '4000' '1' \
--window_scaler=8.0 \
--config_file=./configs/turtlebot_random_nav.yaml \
--device_idx=0 \
--seed=15 > nohup1.out &

# real-time
$ nohup python -u test_pfnet.py \
--pfnet_loadpath=./pfnetwork/checkpoints/pfnet_igibson_data/checkpoint_63_0.136/pfnet_checkpoint \
--obs_mode='rgb-depth' \
--scene_id='Rs' \
--init_particles_distr='gaussian' \
--init_particles_std '0.2' '0.523599' \
--particles_range=100 \
--num_particles=500 \
--transition_std '0.04' '0.0872665' \
--resample=true \
--alpha_resample_ratio=0.95 \
--global_map_size '100' '100' '1' \
--window_scaler=1.0 \
--config_file=./configs/locobot_pfnet_nav.yaml \
--device_idx=0 \
--seed=15 > nohup1.out &

June 9th 2021
-------------
1. collect random trajectories w.r.t discrete actions and train PFNet
2. collect train/test for 200 houses with trajectory length 100, for 10,000 trajectories
3. then re-eval after training PFNet
4. K-mean representation of particles

$ nohup python -u supervised_data.py \
--filename=./pfnet_data/train/Rs0_floor0.tfrecord \
--scene_id='Rs' \
--agent='random' \
--num_records=5 \
--obs_mode='rgb-depth' \
--config_file=./configs/locobot_pfnet_nav.yaml \
--env_mode='headless' \
--gpu_num=0 \
--init_env_pfnet=False \
--seed=90 > nohup.out &

0: --seed=136
1: --seed=489
2: --seed=1728
3: --seed=910
$ nohup python -u train_pfnet.py \
--root_dir=./run1 \
--tfrecordpath=./pfnet_data \
--epochs=100 \
--obs_mode='rgb-depth' \
--num_train_samples=4000 \
--num_eval_samples=500 \
--batch_size=12 \
--s_buffer_size=500 \
--pfnet_loadpath='' \
--learning_rate=5e-5 \
--init_particles_distr='gaussian' \
--init_particles_std '0.15' '0.523599' \
--particles_range=100 \
--num_particles=30 \
--transition_std '0.' '0.' \
--resample=false \
--alpha_resample_ratio=0.5 \
--global_map_size '100' '100' '1' \
--window_scaler=1.0 \
--config_file=./configs/locobot_pfnet_nav.yaml \
--device_idx=1 \
--multiple_gpus=false \
--seed=42 > nohup.out &

max_step: 100
task_obs_dim: 18 # proprio + 3 (est_pose)

$ nohup python -u train_eval.py \
--root_dir=train_rl_uniform \
--num_iterations=250000 \
--initial_collect_steps=500 \
--collect_steps_per_iteration=1 \
--num_parallel_environments=1 \
--num_parallel_environments_eval=1 \
--replay_buffer_capacity=2500 \
--train_steps_per_iteration=1 \
--batch_size=1 \
--use_tf_functions=False \
--use_parallel_envs=False \
--num_eval_episodes=10 \
--eval_interval=5000 \
--eval_only=False \
--eval_deterministic=False \
--gpu_c=0 \
--is_localize_env=True \
--config_file=./configs/locobot_pfnet_nav.yaml \
--env_mode=headless \
--gpu_g=0 \
--init_env_pfnet=True \
--init_particles_distr='uniform' \
--init_particles_std=0.4,0.523599 \
--particles_range=100 \
--num_particles=500 \
--resample=True \
--alpha_resample_ratio=0.9 \
--transition_std=0.04,0.0872665 \
--obs_mode='rgb-depth' \
--num_clusters=10 \
--global_map_size=1000,1000,1 \
--pfnet_load=./pfnetwork/checkpoints/pfnet_igibson_data/final/Rs_aprt/checkpoint_242_0.041/pfnet_checkpoint \
--use_plot=False \
--store_plot=False \
--seed=100 > nohup_train_rl_uniform.out &

$ nohup python -u test_rl_agent.py \
--root_dir=train_rl_uniform \
--obs_mode='rgb-depth' \
--num_eval_episodes=10 \
--agent='sac_agent' \
--eval_deterministic=True \
--is_localize_env=True \
--config_file=./configs/locobot_pfnet_nav.yaml \
--gpu_num=0 \
--init_env_pfnet=True \
--init_particles_distr='uniform' \
--init_particles_std=0.4,0.523599 \
--particles_range=100 \
--num_particles=500 \
--resample=True \
--alpha_resample_ratio=0.9 \
--transition_std=0.04,0.0872665 \
--obs_mode='rgb-depth' \
--num_clusters=10 \
--global_map_size=1000,1000,1 \
--pfnet_load=./pfnetwork/checkpoints/pfnet_igibson_data/final/Rs_aprt/checkpoint_242_0.041/pfnet_checkpoint \
--use_plot=True \
--store_plot=True \
--seed=100 > nohup_test_rl_uniform.out &

June 23rd 2021
--------------
1. train from scratch for single sequence - to check anything works
2. plot the trajectories on obstacle map to verify how diverse trajectories are
3. cross check for original dataset
4. different learning rate

$ nohup python -u display_pfnet_data.py \
--root_dir=./run1 \
--tfrecordpath=./Rs_data \
--obs_mode='rgb-depth' \
--num_train_samples=500 \
--batch_size=1 \
--s_buffer_size=50 \
--init_particles_distr='gaussian' \
--init_particles_std '0.2' '0.523599' \
--particles_range=100 \
--num_particles=30 \
--transition_std '0.04' '0.0872665' \
--resample=false \
--alpha_resample_ratio=0.5 \
--global_map_size '100' '100' '1' \
--window_scaler=1.0 \
--config_file=./configs/locobot_pfnet_nav.yaml \
--device_idx=0 \
--seed=42 > nohup.out &

June 30th 2021
--------------
1. interactively check trained pfnet performance in real-time
2. check for over-fitting for train data
3. k-means representation of particles for rl agent training
4. reproduce pfnet paper results for RGB-D then proceed to igibson


valid.tfrecords: 830 records
test.tfrecords: 820 records
train.tfrecords: 74800 records

$ nohup python -u train.py \
--logpath='rgb_depth' \
--trainfiles './house3d_data/train/train-001.tfrecords' \
--evalfiles './house3d_data/eval/valid.tfrecords' \
--num_train_samples=7500 \
--num_eval_samples=816 \
--obs_mode='rgb-depth' \
--init_particles_distr='tracking' \
--init_particles_std '0.15' '0.523599' \
--trajlen=24 \
--num_particles=30 \
--transition_std '0.' '0.' \
--resample=false \
--batch_size=6 \
--learningrate=1e-4 \
--epochs=100 \
--gpu_num=1 \
--seed=42 > rgb_depth.out &

$ nohup python -u display_data.py \
--testfiles './house3d_data/eval/valid.tfrecords' \
--obs_mode='rgb-depth' \
--init_particles_distr='tracking' \
--init_particles_std '0.15' '0.523599' \
--trajlen=24 \
--num_particles=30 \
--transition_std '0.' '0.' \
--resample=false \
--batch_size=8 \
--gpu_num=0 \
--seed=42 > nohup.out &

$ nohup python -u test.py \
--logpath='rgb_depth' \
--testfiles './house3d_data/test/test.tfrecords' \
--num_test_samples=816 \
--obs_mode='rgb-depth' \
--load='' \
--init_particles_distr='tracking' \
--init_particles_std '0.15' '0.523599' \
--trajlen=24 \
--num_particles=30 \
--transition_std '0.' '0.' \
--resample=false \
--batch_size=6 \
--learningrate=1e-4 \
--epochs=100 \
--gpu_num=1 \
--seed=42 > rgb_depth.out &

July 7th 2021
--------------
1. verify depth data for rgb+depth input
2. change following values
depth_low: 0.5
depth_high: 10.0
vertical_fov: 90
increase camera height in urdf file
3. use motion_planning to collect trajectory data

July 15th 2021
--------------
1. train single trajectory for > 1000 backprop steps(epoch) without noise
2. train for single apartment for > 1000 backprop steps(epoch) with and without noise

July 28th 2021
--------------
1. check if network doing well for train data
2. collect more training data
3. take the network trained on single aprt for rl task: rgbd + obstacle/floor map + k-mean (10 cluster) [x, y, theta, weight]
    1: at xy obstacle/empty
    2: at xy particle position particle weight (sum)
    3: at xy particle position orientation (weighted mean)

August 4th 2021
---------------
train rl with rgbd + particle cluster (in px) + floor_map + mse_reward - for fixed episode length (150 steps) single aprt
1. gaussian vs uniform initial dist
2. particle cluster use previous cluster centers as init guess vs random at each episode step.
3. raw particle + weights vs particle cluster
4. Active neural Localization and Deep Active Localization
5. AMCL vs pfnet with laser scan (optional)


August 10th 2021
----------------

other methods:
1. Differentiable Particle Filters (https://github.com/tu-rbo/differentiable-particle-filters)
2. differentiable_filters (https://github.com/akloss/differentiable_filters)
3. Semi-supervised Differentiable Particle Filters (https://github.com/HaoWen-Surrey/SemiDPF)

4. Active Neural Localization (https://github.com/devendrachaplot/Neural-Localization)
5. Deep Active Localization (https://github.com/montrealrobotics/dal)

1. per modality [rgb, depth, rgbd, lidar scan] to show same performance for igibson env as with paper's results.
  train particle filter net for ~100 apartments with multiple floors (i.e ~100k trajectories each 150 env steps)
  batch_size:4, gpu:12gb, ram:10gb, num_particles:50, train_epochs: 50 (per epoch use all data) -> 6 days training time + 1 day testing time (with ablation study)
  ablation: gaussion vs uniform init dist with multiple covariance, 500 vs 1000 particles, multiple re-sampling rate
  fine-tuning vs training from scratch vs out of box
2. train SAC rl agent with above filter for 1 apartment for reward(mse error + collision penality) obs(particle rep + floor map + rgbd)
  batch_size:4, gpu:6gb, ram:24gb, num_particles:500, train_epochs: 100k -> 3 days training time + testing time
  different representations: kmeans cluster center vs raw particles vs local floor map (particle centered)
  ablation: gaussion vs uniform init dist with multiple covariance, 500 vs 1000 particles, with and without rgbd obs, particle efficiency vs env step efficiency

  Deep active localization and Active neural localization implementation check


  August 11th 2021
  ----------------

1. log collisions
2. collision penality: -1.0
3. initialize uniform dist with offset to gt pose
4. 100 x 100 px from max range of lidar around robot
5. get tf1 working on pearl8

channel
1: at xy obstacle/empty (0/1)
2: at xy particle position particle weight (sum)
3: at xy particle position orientation (weighted mean)


Setup Instructions:
------------------
1. Install miniconda from (https://docs.conda.io/en/latest/miniconda.html)
2. Create conda environment
  $ conda create -n py3-igibson python=3.7 anaconda
  $ pip install --upgrade pip
  $ pip install tensorflow
3. Setup igibson
  $ git clone https://github.com/StanfordVL/iGibson --recursive --branch ig-develop
  $ cd iGibson
  $ source activate py3-igibson
  $ pip install -e .
4. add (lidar sensor) scan_link and scan_joint iGibson/igibson/data/assets/models/locobot/locobot.urdf
  refer turtlebot.urdf
